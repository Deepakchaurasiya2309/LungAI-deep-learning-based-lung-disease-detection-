{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omtbRTff-zOO"
      },
      "outputs": [],
      "source": [
        "# PHASE 1: MODEL TRAINING AND SAVING\n",
        "\n",
        "# IMPORTS\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import shutil\n",
        "import random\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# CLASS CONFIGURATIONS\n",
        "CLASSES = ['COVID', 'NORMAL', 'PNEUMONIA', 'TUBERCULOSIS']\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "BASE_DIR = 'data_split'\n",
        "\n",
        "# Drive mounting and path definitions\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = '/content/drive/MyDrive/NNDL_CXR_Project' # Folder for saved model\n",
        "MODEL_FILENAME = os.path.join(DRIVE_PATH, 'resnet_cxr_multidisease_model.h5')\n",
        "RAW_DATA_PATH = '/content/drive/MyDrive/xray_img' # INPUT DATASET\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True) # Ensure the output directory for the model exists\n",
        "\n",
        "# 1)Data Preparation and Splitting\n",
        "\n",
        "def setup_data_directories():\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        for class_name in CLASSES:\n",
        "            os.makedirs(os.path.join(BASE_DIR, split, class_name), exist_ok=True)\n",
        "    print(\"Required data directories (train/val/test) created in:\", BASE_DIR)\n",
        "\n",
        "def split_data_into_folders(raw_data_dir, classes, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    print(\"\\n--- Starting Data Split (70/15/15) ---\")\n",
        "    setup_data_directories()\n",
        "\n",
        "    if os.path.exists(os.path.join(BASE_DIR, 'train', classes[0])) and len(os.listdir(os.path.join(BASE_DIR, 'train', classes[0]))) > 5:\n",
        "        print(\"Data already appears to be split and copied. Skipping split function.\")\n",
        "        return\n",
        "\n",
        "    for class_name in classes:\n",
        "        source_dir = os.path.join(raw_data_dir, class_name)\n",
        "        if not os.path.exists(source_dir):\n",
        "            print(f\"FATAL ERROR: Source directory not found for class '{class_name}'.\")\n",
        "            print(f\"Please check your Google Drive path: {source_dir}\")\n",
        "            return\n",
        "\n",
        "        all_files = [f for f in os.listdir(source_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        if not all_files:\n",
        "            print(f\"No image files found in {source_dir}. Skipping class.\")\n",
        "            continue\n",
        "\n",
        "        train_files, temp_files = train_test_split(all_files, train_size=train_ratio, random_state=42)\n",
        "        val_files, test_files = train_test_split(temp_files, test_size=test_ratio/(val_ratio + test_ratio), random_state=42)\n",
        "\n",
        "        print(f\"Class {class_name}: Total={len(all_files)}, Train={len(train_files)}, Val={len(val_files)}, Test={len(test_files)}\")\n",
        "\n",
        "        for f in train_files:\n",
        "            shutil.copy(os.path.join(source_dir, f), os.path.join(BASE_DIR, 'train', class_name, f))\n",
        "        for f in val_files:\n",
        "            shutil.copy(os.path.join(source_dir, f), os.path.join(BASE_DIR, 'val', class_name, f))\n",
        "        for f in test_files:\n",
        "            shutil.copy(os.path.join(source_dir, f), os.path.join(BASE_DIR, 'test', class_name, f))\n",
        "\n",
        "    print(\"--- Data Splitting and Copying Complete ---\")\n",
        "\n",
        "# Executing Data Split\n",
        "split_data_into_folders(RAW_DATA_PATH, CLASSES)\n",
        "\n",
        "# ImageDataGenerator setup\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, rotation_range=20, width_shift_range=0.1, height_shift_range=0.1,\n",
        "    shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest'\n",
        ")\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Generator Execution\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(BASE_DIR, 'train'), target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', classes=CLASSES,\n",
        ")\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    os.path.join(BASE_DIR, 'val'), target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', classes=CLASSES,\n",
        ")\n",
        "\n",
        "# 2)Model Definition and Class Weight Calculation\n",
        "\n",
        "def calculate_class_weights(generator):\n",
        "    counter = Counter(generator.classes)\n",
        "    total = float(sum(counter.values()))\n",
        "    max_frequency = float(max(counter.values()))\n",
        "\n",
        "    # Calculate initial weights based on inverse class frequency\n",
        "    initial_weights = {cls_idx: total / (len(counter) * count) for cls_idx, count in counter.items()}\n",
        "\n",
        "    # Normalizing weights\n",
        "    max_weight = max(initial_weights.values())\n",
        "    class_weights = {cls_idx: weight / max_weight for cls_idx, weight in initial_weights.items()}\n",
        "\n",
        "    # Scale all weights by a factor (e.g., 2 or 3) to emphasize rare classes more\n",
        "    scaling_factor = 2.0\n",
        "    class_weights = {cls_idx: weight * scaling_factor for cls_idx, weight in class_weights.items()}\n",
        "\n",
        "    print(f\"\\n--- Class Distribution (by index: {generator.class_indices}) ---\")\n",
        "    for cls_idx, count in counter.items():\n",
        "        print(f\"Class {CLASSES[cls_idx]} ({cls_idx}): {count} samples. Weight: {class_weights[cls_idx]:.2f}\")\n",
        "\n",
        "    return class_weights\n",
        "\n",
        "def build_resnet_model(num_classes, trainable_layers=0, learning_rate=1e-3):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # Set trainable layers based on the number of layers in the model\n",
        "    if trainable_layers == 0:\n",
        "        for layer in base_model.layers:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "        # Unfreeze the top layers for fine-tuning\n",
        "        for layer in base_model.layers[:-trainable_layers]:\n",
        "            layer.trainable = False\n",
        "        for layer in base_model.layers[-trainable_layers:]:\n",
        "            layer.trainable = True\n",
        "\n",
        "    # Check if the head needs to be added (only add once)\n",
        "    if not base_model.output.shape[-1] == num_classes:\n",
        "        x = base_model.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dense(1024, activation='relu')(x)\n",
        "        predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
        "        model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    else:\n",
        "        model = base_model\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy',\n",
        "                           tf.keras.metrics.Precision(),\n",
        "                           tf.keras.metrics.Recall(),\n",
        "                           tf.keras.metrics.AUC(name='auc')])\n",
        "    return model\n",
        "\n",
        "# 3)Two-Stage Training and Saving Model\n",
        "\n",
        "def train_and_save_model():\n",
        "\n",
        "    class_weights = calculate_class_weights(train_generator)\n",
        "\n",
        "    # STAGE 1: Train Classification Head (Fast Learning Rate)\n",
        "    print(\"\\n\\n--- STAGE 1: Training Classification Head (Fast) ---\")\n",
        "    model = build_resnet_model(NUM_CLASSES, trainable_layers=0, learning_rate=1e-3)\n",
        "\n",
        "    checkpoint_stage1 = ModelCheckpoint(MODEL_FILENAME, monitor='val_auc', save_best_only=True, mode='max', verbose=1)\n",
        "    early_stop_stage1 = EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True, mode='max')\n",
        "\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "        epochs=15, # Shorter epochs for head training\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=val_generator.samples // BATCH_SIZE,\n",
        "        callbacks=[checkpoint_stage1, early_stop_stage1],\n",
        "        class_weight=class_weights # Use weights here to calibrate initial head\n",
        "    )\n",
        "\n",
        "    # Loading the best model from Stage 1 before fine-tuning\n",
        "    best_stage1_model = load_model(MODEL_FILENAME)\n",
        "\n",
        "    # STAGE 2: Fine-Tuning Last Block (Slow Learning Rate + Weights)\n",
        "    print(\"\\n\\n--- STAGE 2: Fine-Tuning Last 30 Layers (Slow) ---\")\n",
        "    # 30 layers corresponds roughly to the last major block of ResNet50\n",
        "    layers_to_unfreeze = 30\n",
        "\n",
        "    # Re-compile the best model from Stage 1 with unfrozen layers and very low LR\n",
        "    model = build_resnet_model(NUM_CLASSES, trainable_layers=layers_to_unfreeze, learning_rate=1e-5)\n",
        "\n",
        "    # Transfer weights from the best model of Stage 1\n",
        "    model.set_weights(best_stage1_model.get_weights())\n",
        "\n",
        "    # Define new callbacks for Stage 2\n",
        "    checkpoint_stage2 = ModelCheckpoint(MODEL_FILENAME, monitor='val_auc', save_best_only=True, mode='max', verbose=1)\n",
        "    early_stop_stage2 = EarlyStopping(monitor='val_auc', patience=10, restore_best_weights=True, mode='max')\n",
        "\n",
        "    print(f\"Starting Fine-Tuning for {50} epochs (or until Early Stopping)...\")\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "        epochs=50, # More epochs for fine-tuning\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=val_generator.samples // BATCH_SIZE,\n",
        "        callbacks=[checkpoint_stage2, early_stop_stage2],\n",
        "        class_weight=class_weights # Using weights to improve balanced accuracy\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining Complete. FINAL best model saved persistently to Google Drive: {MODEL_FILENAME}\")\n",
        "\n",
        "# 4)Model Evaluation on Test Set\n",
        "\n",
        "def evaluate_saved_model_on_test_data():\n",
        "    print(\"\\n--- Starting Test Set Evaluation ---\")\n",
        "    try:\n",
        "        best_model = load_model(MODEL_FILENAME)\n",
        "        print(f\"Successfully loaded best model from {MODEL_FILENAME}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load the model. Ensure training ran successfully. Error: {e}\")\n",
        "        return\n",
        "\n",
        "    # Creating Test Generator\n",
        "    test_generator = val_test_datagen.flow_from_directory(\n",
        "        os.path.join(BASE_DIR, 'test'),\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical',\n",
        "        classes=CLASSES,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    print(\"\\nEvaluating model on the final, unseen test dataset...\")\n",
        "    metrics = best_model.evaluate(test_generator, steps=test_generator.samples // BATCH_SIZE)\n",
        "\n",
        "    metric_names = ['Loss', 'Accuracy', 'Precision', 'Recall', 'AUC']\n",
        "    print(\"-\" * 40)\n",
        "    print(\"Final Test Set Performance:\")\n",
        "    print(\"-\" * 40)\n",
        "    for name, value in zip(metric_names, metrics):\n",
        "        if name == 'Loss':\n",
        "            print(f\"{name:<10}: {value:.4f}\")\n",
        "        else:\n",
        "            style = '**' if name in ['Accuracy', 'AUC'] else ''\n",
        "            print(f\"{name:<10}: {style}{value * 100:.2f}%{style}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if metrics[1] < 0.85:\n",
        "        print(\"ðŸ’¡ NOTE: Accuracy is still below target. Fine-tuning more layers or using more data augmentation may be needed.\")\n",
        "    else:\n",
        "        print(\"âœ… SUCCESS: Test Accuracy is high, indicating good model generalization and no major overfitting.\")\n",
        "\n",
        "    print(\"You can now proceed to Phase 2 (Block 2), even on a CPU runtime.\")\n",
        "\n",
        "\n",
        "# TRAINING AND SAVING EXECUTION\n",
        "train_and_save_model()\n",
        "\n",
        "# FINAL TEST EVALUATION\n",
        "evaluate_saved_model_on_test_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-keras-vis gradio"
      ],
      "metadata": {
        "id": "FCc6F5bi-6yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL LOADING AND GRADIO INTEGRATION\n",
        "\n",
        "# IMPORTS\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from google.colab import drive # For mounting drive\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from tf_keras_vis.gradcam import Gradcam\n",
        "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
        "\n",
        "\n",
        "# CLASS CONFIGURATION\n",
        "CLASSES = ['COVID', 'NORMAL', 'PNEUMONIA', 'TUBERCULOSIS']\n",
        "IMG_SIZE = 224\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = '/content/drive/MyDrive/NNDL_CXR_Project' # Path of saved model\n",
        "MODEL_FILENAME = os.path.join(DRIVE_PATH, 'resnet_cxr_multidisease_model.h5')\n",
        "\n",
        "# 4)Grad-CAM Visualisation Module\n",
        "\n",
        "def visualize_gradcam(model, input_image_array, target_index):\n",
        "    modifier = ReplaceToLinear()\n",
        "    def loss_function(output):\n",
        "        return tf.cast(output[0][target_index], tf.float32)\n",
        "\n",
        "    # Instantiate Gradcam model and Heat-map computation\n",
        "    model_for_gradcam = tf.keras.models.clone_model(model)\n",
        "    model_for_gradcam.set_weights(model.get_weights())\n",
        "\n",
        "    gradcam = Gradcam(model_for_gradcam, model_modifier=modifier, clone=True)\n",
        "    cam = gradcam(loss_function, input_image_array, penultimate_layer=-1)\n",
        "    heatmap = np.uint8(255 * cam[0])\n",
        "\n",
        "    # Original image preparation for overlay\n",
        "    img = Image.fromarray((input_image_array[0] * 255).astype(np.uint8))\n",
        "    img = np.array(img.resize((IMG_SIZE, IMG_SIZE)))\n",
        "\n",
        "    # Overlaying heatmap onto the original image\n",
        "    heatmap_colored = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    superimposed_img = heatmap_colored * 0.4 + img * 0.6 # Blending\n",
        "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return Image.fromarray(superimposed_img)\n",
        "\n",
        "# 5)Gradio Interface Logic and Prediction Function\n",
        "\n",
        "def classify_xray(input_img_pil):\n",
        "    \"\"\"\n",
        "    Handles image processing, prediction, Grad-CAM generation, and output formatting.\n",
        "    \"\"\"\n",
        "    # 1)Load Model\n",
        "    try:\n",
        "        # Loading the saved model with the custom AUC metric included\n",
        "        print(f\"Loading model from: {MODEL_FILENAME}\")\n",
        "        model = load_model(MODEL_FILENAME, custom_objects={'auc': tf.keras.metrics.AUC(name='auc')})\n",
        "    except Exception as e:\n",
        "        return None, f\"<h1>Model Load Error</h1><p>Could not load model from Drive path: '{MODEL_FILENAME}'. **Did you run Phase 1 (Block 1) on GPU and mount Google Drive?** Error: {e}</p>\", \"\"\n",
        "\n",
        "    # Preprocessing image\n",
        "    img = input_img_pil.resize((IMG_SIZE, IMG_SIZE))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) / 255.0 # Normalizing\n",
        "\n",
        "    # Prediction Making\n",
        "    predictions = model.predict(img_array)[0]\n",
        "\n",
        "    # Getting the top prediction details\n",
        "    predicted_index = np.argmax(predictions)\n",
        "    predicted_class = CLASSES[predicted_index]\n",
        "    confidence = predictions[predicted_index] * 100\n",
        "\n",
        "    # Generation of Grad-CAM visualization\n",
        "    gradcam_img = visualize_gradcam(model, img_array, predicted_index)\n",
        "\n",
        "    # HTML Output and Alert Message(UI Part) Generation\n",
        "\n",
        "    # Defining Alert Messages based on Predicted Class and Confidence level\n",
        "    if predicted_class != 'NORMAL' and confidence >= 90:\n",
        "        alert_style = 'background-color: #ffe5e5; border: 3px solid #f00; color: #a30000; padding: 15px; border-radius: 10px; font-weight: 700; box-shadow: 0 4px 8px rgba(0,0,0,0.1);'\n",
        "        alert_message = f\"ðŸ”´ CRITICAL ALERT: High likelihood of {predicted_class} (Confidence: {confidence:.1f}%). Immediate clinical correlation and confirmatory testing are **strongly recommended**.\"\n",
        "\n",
        "    elif predicted_class != 'NORMAL' and confidence >= 50:\n",
        "        alert_style = 'background-color: #fff8e1; border: 3px solid #ffc107; color: #8d6e30; padding: 15px; border-radius: 10px; font-weight: 600;'\n",
        "        alert_message = f\"âš ï¸ ELEVATED CONCERN: Moderate evidence of lung pathology detected. Moderate likelihood of {predicted_class} (Confidence: {confidence:.1f}%). Suggests a need for close follow-up and specialist review.\"\n",
        "\n",
        "    elif predicted_class == 'NORMAL' and confidence >= 90:\n",
        "        alert_style = 'background-color: #e6f7e6; border: 3px solid #4caf50; color: #2e7d32; padding: 15px; border-radius: 10px;'\n",
        "        alert_message = f\"âœ… NO ACUTE PATHOLOGY DETECTED: Model predicts a Normal/Healthy condition (Confidence: {confidence:.1f}%). Please consult a medical professional for final assessment.\"\n",
        "\n",
        "    elif predicted_class == 'NORMAL' and confidence >= 50:\n",
        "        alert_style = 'background-color: #f0fff4; border: 1px solid #90ee90; color: #388e3c; padding: 15px; border-radius: 10px; font-weight: 600;'\n",
        "        alert_message = f\"ðŸŸ¢ Model suggests Moderate likelihood of Normal Condition (Confidence: {confidence:.1f}%). Due to moderate certainty, routine follow-up or a quick clinical is advised.\"\n",
        "\n",
        "    else:\n",
        "        alert_style = 'background-color: #f7fafc; border: 1px solid #e2e8f0; color: #4a5568; padding: 10px; border-radius: 8px;'\n",
        "        alert_message = \"General screening result. Review all scores and consult a medical professional.\"\n",
        "\n",
        "    alert_html = f\"<div style='{alert_style}'>{alert_message}</div>\"\n",
        "\n",
        "    # 2)Final Results Table(Showing the confidence scores)\n",
        "    results_html = f\"<h2 style='color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px;'>ðŸ©º Multi-Disease Screening Report</h2>\"\n",
        "    results_html += f\"<div style='margin-bottom: 25px; padding: 15px; border-radius: 12px; background-color: #3498db; color: white; box-shadow: 0 4px 6px rgba(0,0,0,0.1);'>\"\n",
        "    results_html += f\"<h3 style='margin: 0;'>Primary Diagnosis: {predicted_class} ({confidence:.1f}%)</h3>\"\n",
        "    results_html += \"</div>\"\n",
        "\n",
        "    results_html += \"<h4 style='color: #2c3e50;'>Confidence Scores:</h4>\"\n",
        "    results_html += \"<table style='width: 100%; border-collapse: separate; border-spacing: 0 10px; font-size: 15px;'>\"\n",
        "\n",
        "    scores_list = sorted([(CLASSES[i], p * 100) for i, p in enumerate(predictions)], key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    for condition, score in scores_list:\n",
        "        bg_color = '#e9f5ff' if condition == predicted_class else '#f8f9fa'\n",
        "        text_color = '#2980b9' if condition == predicted_class else '#34495e'\n",
        "        font_weight = 'bold' if condition == predicted_class else 'normal'\n",
        "\n",
        "        results_html += f\"<tr style='background-color: {bg_color}; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.05);'>\"\n",
        "        results_html += f\"<td style='padding: 10px; border-radius: 8px 0 0 8px; color: {text_color}; font-weight: {font_weight};'>{condition}</td>\"\n",
        "        results_html += f\"<td style='padding: 10px; border-radius: 0 8px 8px 0; text-align: right; color: {text_color}; font-weight: {font_weight};'>{score:.2f}%</td></tr>\"\n",
        "\n",
        "    results_html += \"</table>\"\n",
        "\n",
        "    # 3)Final Disclaimer Section\n",
        "    disclaimer_html = f\"<p style='margin-top: 30px; font-size: 12px; color: #7f8c8d; border-top: 1px dashed #ecf0f1; padding-top: 10px;'>**Disclaimer:** This is a **Computer-Aided Diagnosis (CAD)** tool intended for rapid screening purposes only. **Final medical diagnosis must be made by a qualified radiologist.**</p>\"\n",
        "\n",
        "    final_html_output = alert_html + results_html + disclaimer_html\n",
        "\n",
        "    return gradcam_img, final_html_output, predicted_class\n",
        "\n",
        "# 6)Gradio Interface\n",
        "\n",
        "# I/p & O/p definition\n",
        "input_image = gr.Image(type=\"pil\", label=\"Upload Chest X-ray Image\")\n",
        "output_gradcam = gr.Image(type=\"pil\", label=\"Grad-CAM Visualization (Model Evidence)\")\n",
        "output_html = gr.HTML(label=\"Diagnostic Report and Alert\")\n",
        "output_text = gr.Text(label=\"Predicted Class (For Reference)\", visible=False)\n",
        "\n",
        "# Interface definition\n",
        "iface = gr.Interface(\n",
        "    fn=classify_xray,\n",
        "    inputs=input_image,\n",
        "    outputs=[output_gradcam, output_html, output_text],\n",
        "    title=\"ðŸ©º LungAI: Deep Learning-Based Lung Disease Detection\",\n",
        "    description=\"A Deep Learning system utilizing Convolutional Image Classification and Grad-CAM for the rapid and simultaneous detection of four major lung conditions (Normal, Pneumonia, COVID-19, Tuberculosis).\",\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "# GRADIO INTERFACE LAUCH\n",
        "print(\"\\n Project is ready! Loading model and launching Gradio Interface... \")\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "4Ez1Ty-4-_R5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}